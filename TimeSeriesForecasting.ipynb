{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lokesh-Project/Time-Series-Forecasting/blob/main/TimeSeriesForecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "Z_vAxT1WNA-D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This function downloads and extracts the dataset to the directory that contains this file."
      ],
      "metadata": {
        "id": "p8CXg-rUNLw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()"
      ],
      "metadata": {
        "id": "crsqwVxnNK7C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This function normalizes the dataset using min max scaling."
      ],
      "metadata": {
        "id": "C1oFmZcQNqKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data"
      ],
      "metadata": {
        "id": "EKT5CUQdNqfv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This function is used to map the time series dataset into windows of features and respective targets, to prepare it for training and validation."
      ],
      "metadata": {
        "id": "84dCWeZHNriv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "gdyndoUgNqwo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This function loads the data from CSV file, normalizes the data and splits the dataset into train and validation data. It also uses windowed_dataset() to split the data into windows of observations and targets. Finally, it defines, compiles and trains a neural network. This function returns the final trained model."
      ],
      "metadata": {
        "id": "xGvnrvBNOzUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "    # Reads the dataset from the CSV.\n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features at future time steps.\n",
        "    N_FEATURES = len(df.columns)\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.5)\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # Clear any previous models from memory.\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Number of past time steps based on which future observations should be predicted.\n",
        "    N_PAST = 24\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 24\n",
        "\n",
        "    # By how many positions the window slides to create a new window of observations.\n",
        "    SHIFT = 1\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE, n_past=N_PAST, n_future=N_FUTURE, shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE, n_past=N_PAST, n_future=N_FUTURE, shift=SHIFT)\n",
        "\n",
        "    # Code to define the model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=(N_PAST, N_FEATURES)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "        tf.keras.layers.RepeatVector(N_FUTURE),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(N_FEATURES))\n",
        "    ])\n",
        "\n",
        "    # Code to compile the model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Code to train the model\n",
        "    model.fit(train_set, validation_data=valid_set, epochs=10)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "PGL7HQf_ODOi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sye_x_BZvJT",
        "outputId": "a1a896cb-664d-4793-d296-b50b45c8c131"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-85355796102e>:6: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  df = pd.read_csv('household_power_consumption.csv', sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1349/1349 [==============================] - 210s 144ms/step - loss: 0.0188 - val_loss: 0.0158\n",
            "Epoch 2/10\n",
            "1349/1349 [==============================] - 190s 141ms/step - loss: 0.0148 - val_loss: 0.0138\n",
            "Epoch 3/10\n",
            "1349/1349 [==============================] - 189s 140ms/step - loss: 0.0137 - val_loss: 0.0135\n",
            "Epoch 4/10\n",
            "1349/1349 [==============================] - 229s 170ms/step - loss: 0.0133 - val_loss: 0.0126\n",
            "Epoch 5/10\n",
            "1349/1349 [==============================] - 190s 141ms/step - loss: 0.0128 - val_loss: 0.0121\n",
            "Epoch 6/10\n",
            "1349/1349 [==============================] - 189s 140ms/step - loss: 0.0125 - val_loss: 0.0119\n",
            "Epoch 7/10\n",
            "1349/1349 [==============================] - 190s 141ms/step - loss: 0.0123 - val_loss: 0.0119\n",
            "Epoch 8/10\n",
            "1349/1349 [==============================] - 188s 139ms/step - loss: 0.0123 - val_loss: 0.0118\n",
            "Epoch 9/10\n",
            "1349/1349 [==============================] - 190s 141ms/step - loss: 0.0121 - val_loss: 0.0117\n",
            "Epoch 10/10\n",
            "1349/1349 [==============================] - 189s 140ms/step - loss: 0.0119 - val_loss: 0.0118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}